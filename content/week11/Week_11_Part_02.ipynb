{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support embedding YouTube Videos in Notebooks\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Part 2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course tries to give you visual or intuitive interpretations of many concepts. As you get deeper into the theory, however, it helps to see whether some more abstract mathematical ideas can help us understand probability distributions. One such is the *moment generating function*.\n",
    "\n",
    "We are studying it at this stage because it has powerful applications to distributions of sums of independent random variables.\n",
    "\n",
    "### Moments of $X$ ###\n",
    "$E(X)$ is the center of gravity of the distribution of $X$. In physics, the calculation of center of gravity relies on the *principle of moments*. So $E(X)$ is called the *first moment* of $X$. In general:\n",
    "\n",
    "- For $k = 1, 2, 3, \\ldots$, the *$k$th moment* of $X$ is $E(X^k)$.\n",
    "\n",
    "If $X$ can have arbitrarily large values, either positive or negative, then $E(X^k)$ need not exist. But for our purposes it's fine to assume that all moments exist.\n",
    "\n",
    "- Note that $Var(X) = E(X^2) - (E(X))^2$ is a function of the first and second moments.\n",
    "\n",
    "Understanding the moments helps us understand the tails of a distribution.\n",
    "\n",
    "### Markov and Chebyshev ###\n",
    "- Markov's inequality provides a tail bound (for non-negative random variables) using only the first moment.\n",
    "- Chebyshev's inequality provides a tail bound using the first two moments.\n",
    "\n",
    "In cases where both bounds apply, Chebyshev often does better than Markov because it uses the variance which involves the second moment as well as the first. \n",
    "\n",
    "So it is reasonable to think that the more moments you know, the closer you can get to the tail probabilities. That is indeed the case: typically, if you know all the moments then you know the entire distribution.\n",
    "\n",
    "Moment generating functions make it possible for us to work with all the moments at once. They are math functions that contain a remarkable amount of information about probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: darkblue\">Moment Generating Function</span> #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/8MeU79QdChA\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10ea25550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo(\"8MeU79QdChA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading: MGF and Tails ###\n",
    "Start with the [definition](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html) (ignore references to the probability generating function). \n",
    "\n",
    "Note that if $M$ is a moment generating function then $M(0) = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/_zwnCq3T9zo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10ea5c240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo(\"_zwnCq3T9zo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go over the [relation](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#Note-on-Existence) between the tails of the distribution and the existence of the mgf at a fixed $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What follows is a list of properties and calculations. Each individual one is quite do-able. Take your time and do the details.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: darkblue\">Properties</span> #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these properties assume that the moment generating function exists in an interval around $0$. You don't have to worry about that in this course.\n",
    "\n",
    "### 1. Reading: Generating Moments ###\n",
    "\n",
    "The $n$th moment of $X$ is the [$n$th derivative](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#Generating-Moments) of $M_X$ at 0.\n",
    "\n",
    "$$\n",
    "M_X^{(n)}(0) = E(X^n)\n",
    "$$\n",
    "\n",
    "For example, if you have the mgf of $X$ then:\n",
    "\n",
    "- Take the derivative, evaluate it at 0, and you have $E(X)$.\n",
    "- Take the second derivative, evaluate it at 0, and you have $E(X^2)$.\n",
    "- Now use $Var(X) = E(X^2) - (E(X))^2$ to get the variance.\n",
    "\n",
    "### 2. Linear Transformation ###\n",
    "\n",
    "$$\n",
    "M_{aX+b}(t) ~ = ~ E(e^{t(aX+b)}) ~ = ~ e^{bt}E(e^{atX}) ~ = ~ e^{bt}M_X(at)\n",
    "$$\n",
    "\n",
    "### 3. Reading: Identifying the Distribution ###\n",
    "If you just know the expectation of $X$, or the expectation and the SD, you don't know the distribution of $X$. But if you know the mgf of $X$, then you [know the distribution](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#Identifying-the-Distribution). We won't prove this, but we will use it to recognize distributions.\n",
    "\n",
    "### 4. Reading: MGFs and Sums ###\n",
    "A [crucial property](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#Working-Well-with-Sums) for independent $X$ and $Y$:\n",
    "\n",
    "$$\n",
    "M_{X+Y}(t) ~ = ~ M_X(t)M_Y(t)\n",
    "$$\n",
    "\n",
    "It implies that if $X_1, X_2, \\ldots, X_n$ are i.i.d. and $S_n = X_1 + X_2 + \\cdots + X_n$, then\n",
    "\n",
    "$$\n",
    "M_{S_n}(t) ~ = ~ (M_{X_1}(t))^n\n",
    "$$\n",
    "\n",
    "This allows us to work with distributions of sums when the convolution formula proves to be intractable, which happens pretty often. It also helps prove the CLT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: darkblue\">Calculating the MGF</span> #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_X(t)$ is the expectation of a function of $X$. To calculate $M_X(t)$, first check whether you can apply any of the properties above. If you can't, you might have to use the definition.\n",
    "\n",
    "**Discrete $X$**\n",
    "$$\n",
    "M_X(t) ~ = ~ E(e^{tX}) ~ = ~ \\sum_x e^{tx}P(X=x)\n",
    "$$\n",
    "\n",
    "**$X$ with Density $f$**\n",
    "$$\n",
    "M_X(t) ~ = ~ E(e^{tX}) ~ = ~ \\int_{-\\infty}^\\infty e^{tx}f(x)dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: darkblue\">Distributions of Sums</span> #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these calculations line by line; don't just skim.\n",
    "\n",
    "### Reading: Independent Indicators ###\n",
    "- Indicators and the [binomial](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#MGFs-of-Some-Discrete-Random-Variables)\n",
    "- The Poisson is indeed an exercise. See the Checkpoint.\n",
    "\n",
    "### Reading: Independent Gamma Variables ###\n",
    "- Proof that the sum of independent [gamma](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#MGF-of-a-Gamma-$(r,-\\lambda-)$-Random-Variable) variables with the same rate is [also gamma](http://prob140.org/textbook/Chapter_19/02_Moment_Generating_Functions.html#Sums-of-Independent-Gamma-Variables-with-the-Same-Rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/RPTDX6sfIz0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10ea5c208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo(\"RPTDX6sfIz0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading: Independent Normal Variables ###\n",
    "- Proof that the sum of independent [normal](http://prob140.org/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html#MGFs,-the-Normal,-and-the-CLT) variables is [also normal](http://prob140.org/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html#Sums-of-Independent-Normal-Variables).\n",
    "\n",
    "Note the general form of the [normal mgf](http://prob140.org/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html#Normal-$(\\mu,-\\sigma^2)$). It will help you recognize normal distributions.\n",
    "\n",
    "- $e$ to the power of a quadratic in $t$\n",
    "- coefficient of $t^2$ is positive ($\\sigma^2/2$)\n",
    "- there is no constant term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: darkblue\">Proof of CLT (Sort of, and Optional)</span> #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Sort of\" because it relies a reasonable result that we have't proved. But it's as close as you'll get to a proof without going through measure theory and a graduate class.\n",
    "\n",
    "\"Optional\" because this is a weird semester.\n",
    "\n",
    "### Reading (Optional): Proof of CLT by MGFs###\n",
    "It's [here](http://prob140.org/textbook/Chapter_19/03_MGFs_Normal_and_the_CLT.html#%22Proof%22-of-the-Central-Limit-Theorem), and it's fun, but you have to work through it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vitamins ##\n",
    "\n",
    "**1.** The function $h(t) = 2t^2 - t + 5$ for $-\\infty < t < \\infty$ can't be the mgf of any random variable. How can we tell?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer 1</summary>\n",
    "$h(0)$ is wrong for an mgf\n",
    "</details>\n",
    "\n",
    "**2.** The random variable $T$ has mgf $M_T(t) = \\frac{8}{(2-t)^3}$, $t < 2$. What is the distribution of $T$?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer 2</summary>\n",
    "gamma $(3, 2)$\n",
    "</details>\n",
    "\n",
    "**3.** The random variable $X$ has mgf $M_X(t) = e^{10t^2}$ for all $t$. What is the distribution of $X$?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer 3</summary>\n",
    "normal $(0, 20)$\n",
    "</details>\n",
    "\n",
    "**4.** Let $I_1, I_2, \\ldots, I_n$ be independent, and let the distribution of $I_j$ be Bernoulli $(p_j)$. Let $S_n = I_1 + I_2 + \\cdots + I_n$. Find the mgf of $S_n$.\n",
    "\n",
    "[Note that $S_n$ is an extension of the binomial distribution. It is the number of successes in $n$ independent trials that can have different success probabilities. Its distribution is a pain to write down. But its mgf can be used to get tail bounds, which is particularly useful when $n$ is large.]\n",
    "\n",
    "<details>\n",
    "    <summary>Answer 4</summary>\n",
    "$M_{S_n}(t) = \\prod_{j=1}^n (q_j + p_je^t)$ for all $t$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a good time to pause. Exponential bounds coming up. ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
